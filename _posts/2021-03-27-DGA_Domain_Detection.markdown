---
layout: post
title:  "Detecting Algorithmically Generated Domain Names"
date:   2021-03-27 07:00:14 +0000
excerpt_separator: <!--more-->
published: true
---

## Exercise to detect Algorithmically Generated Domain Names.
In this notebook we're going to use some great python modules to explore, understand and classify domains as being 'legit' or having a high probability of being generated by a DGA (Dynamic Generation Algorithm). We have 'legit' in quotes as we're using the domains in Alexa as the 'legit' set. The primary motivation is to explore the nexus of IPython, Pandas and scikit-learn with DGA classification as a vehicle for that exploration. The exercise intentionally shows common missteps, warts in the data, paths that didn't work out that well and results that could definitely be improved upon. In general capturing what worked and what didn't is not only more realistic but often much more informative. :)

### Python Modules Used:
- Pandas: Python Data Analysis Library (http://pandas.pydata.org)
- Scikit Learn (http://scikit-learn.org) Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.
- Matplotlib:  Python 2D plotting library (http://matplotlib.org)

**Suggestions/Comments:**
Please send suggestions or bugs (I'm sure) to *clicklabs at clicksecurity.com*. Also if you have some datasets or would like to explore alternative approaches please touch base.




```
import sklearn.feature_extraction
sklearn.__version__
```




    '0.14.1'




```
import pandas as pd
pd.__version__
```




    '0.12.0'




```
# Set default pylab stuff
pylab.rcParams['figure.figsize'] = (14.0, 5.0)
pylab.rcParams['axes.grid'] = True
```


```
# Version 0.12.0 of Pandas has a DeprecationWarning about Height blah that I'm ignoring
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
```


```
# This is the Alexa 100k domain list, we're not using the 1 Million just for speed reasons. Results
# for the Alexa 1M are given at the bottom of the notebook.
alexa_dataframe = pd.read_csv('data/alexa_100k.csv', names=['rank','uri'], header=None, encoding='utf-8')
alexa_dataframe.head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank</th>
      <th>uri</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> 1</td>
      <td> facebook.com</td>
    </tr>
    <tr>
      <th>1</th>
      <td> 2</td>
      <td>   google.com</td>
    </tr>
    <tr>
      <th>2</th>
      <td> 3</td>
      <td>  youtube.com</td>
    </tr>
    <tr>
      <th>3</th>
      <td> 4</td>
      <td>    yahoo.com</td>
    </tr>
    <tr>
      <th>4</th>
      <td> 5</td>
      <td>    baidu.com</td>
    </tr>
  </tbody>
</table>
</div>




```
# Okay for this exercise we need the 2LD and nothing else
import tldextract

def domain_extract(uri):
    ext = tldextract.extract(uri)
    if (not ext.suffix):
        return np.nan
    else:
        return ext.domain

alexa_dataframe['domain'] = [ domain_extract(uri) for uri in alexa_dataframe['uri']]
del alexa_dataframe['rank']
del alexa_dataframe['uri']
alexa_dataframe.head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> facebook</td>
    </tr>
    <tr>
      <th>1</th>
      <td>   google</td>
    </tr>
    <tr>
      <th>2</th>
      <td>  youtube</td>
    </tr>
    <tr>
      <th>3</th>
      <td>    yahoo</td>
    </tr>
    <tr>
      <th>4</th>
      <td>    baidu</td>
    </tr>
  </tbody>
</table>
</div>




```
alexa_dataframe.tail()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>99995</th>
      <td> rhbabyandchild</td>
    </tr>
    <tr>
      <th>99996</th>
      <td>             rm</td>
    </tr>
    <tr>
      <th>99997</th>
      <td>           sat1</td>
    </tr>
    <tr>
      <th>99998</th>
      <td>     nahimunkar</td>
    </tr>
    <tr>
      <th>99999</th>
      <td>           musi</td>
    </tr>
  </tbody>
</table>
</div>




```
# It's possible we have NaNs from blanklines or whatever
alexa_dataframe = alexa_dataframe.dropna()
alexa_dataframe = alexa_dataframe.drop_duplicates()

# Set the class
alexa_dataframe['class'] = 'legit'

# Shuffle the data (important for training/testing)
alexa_dataframe = alexa_dataframe.reindex(np.random.permutation(alexa_dataframe.index))
alexa_total = alexa_dataframe.shape[0]
print 'Total Alexa domains %d' % alexa_total

# Hold out 10%
hold_out_alexa = alexa_dataframe[alexa_total*.9:]
alexa_dataframe = alexa_dataframe[:alexa_total*.9]

print 'Number of Alexa domains: %d' % alexa_dataframe.shape[0]
```

    Total Alexa domains 91712
    Number of Alexa domains: 82540



```
alexa_dataframe.head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20904</th>
      <td> transworld</td>
      <td> legit</td>
    </tr>
    <tr>
      <th>82690</th>
      <td>      lkfun</td>
      <td> legit</td>
    </tr>
    <tr>
      <th>85167</th>
      <td>  islam2all</td>
      <td> legit</td>
    </tr>
    <tr>
      <th>62859</th>
      <td>   pulitzer</td>
      <td> legit</td>
    </tr>
    <tr>
      <th>85573</th>
      <td>        sge</td>
      <td> legit</td>
    </tr>
  </tbody>
</table>
</div>




```
# Read in the DGA domains
dga_dataframe = pd.read_csv('data/dga_domains.txt', names=['raw_domain'], header=None, encoding='utf-8')

# We noticed that the blacklist values just differ by captilization or .com/.org/.info
dga_dataframe['domain'] = dga_dataframe.applymap(lambda x: x.split('.')[0].strip().lower())
del dga_dataframe['raw_domain']

# It's possible we have NaNs from blanklines or whatever
dga_dataframe = dga_dataframe.dropna()
dga_dataframe = dga_dataframe.drop_duplicates()
dga_total = dga_dataframe.shape[0]
print 'Total DGA domains %d' % dga_total

# Set the class
dga_dataframe['class'] = 'dga'

# Hold out 10%
hold_out_dga = dga_dataframe[dga_total*.9:]
dga_dataframe = dga_dataframe[:dga_total*.9]

print 'Number of DGA domains: %d' % dga_dataframe.shape[0]
```

    Total DGA domains 2664
    Number of DGA domains: 2397



```
dga_dataframe.head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> 04055051be412eea5a61b7da8438be3d</td>
      <td> dga</td>
    </tr>
    <tr>
      <th>1</th>
      <td>                       1cb8a5f36f</td>
      <td> dga</td>
    </tr>
    <tr>
      <th>2</th>
      <td> 30acd347397c34fc273e996b22951002</td>
      <td> dga</td>
    </tr>
    <tr>
      <th>3</th>
      <td> 336c986a284e2b3bc0f69f949cb437cb</td>
      <td> dga</td>
    </tr>
    <tr>
      <th>5</th>
      <td> 40a43e61e56a5c218cf6c22aca27f7ee</td>
      <td> dga</td>
    </tr>
  </tbody>
</table>
</div>




```
# Concatenate the domains in a big pile!
all_domains = pd.concat([alexa_dataframe, dga_dataframe], ignore_index=True)
```


```
# Add a length field for the domain
all_domains['length'] = [len(x) for x in all_domains['domain']]

# Okay since we're trying to detect dynamically generated domains and short
# domains (length <=6) are crazy random even for 'legit' domains we're going
# to punt on short domains (perhaps just white/black list for short domains?)
all_domains = all_domains[all_domains['length'] > 6]
```


```
# Grabbed this from Rosetta Code (rosettacode.org)
import math
from collections import Counter
 
def entropy(s):
    p, lns = Counter(s), float(len(s))
    return -sum( count/lns * math.log(count/lns, 2) for count in p.values())
```


```
# Add a entropy field for the domain
all_domains['entropy'] = [entropy(x) for x in all_domains['domain']]
```


```
all_domains.head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>    transworld</td>
      <td> legit</td>
      <td> 10</td>
      <td> 3.121928</td>
    </tr>
    <tr>
      <th>2</th>
      <td>     islam2all</td>
      <td> legit</td>
      <td>  9</td>
      <td> 2.419382</td>
    </tr>
    <tr>
      <th>3</th>
      <td>      pulitzer</td>
      <td> legit</td>
      <td>  8</td>
      <td> 3.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>   danarimedia</td>
      <td> legit</td>
      <td> 11</td>
      <td> 2.663533</td>
    </tr>
    <tr>
      <th>7</th>
      <td> heartbreakers</td>
      <td> legit</td>
      <td> 13</td>
      <td> 2.815072</td>
    </tr>
  </tbody>
</table>
</div>




```
all_domains.tail()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>84932</th>
      <td> ulxxqduryvv</td>
      <td> dga</td>
      <td> 11</td>
      <td> 2.913977</td>
    </tr>
    <tr>
      <th>84933</th>
      <td>    ummvzhin</td>
      <td> dga</td>
      <td>  8</td>
      <td> 2.750000</td>
    </tr>
    <tr>
      <th>84934</th>
      <td>    umsgnwgc</td>
      <td> dga</td>
      <td>  8</td>
      <td> 2.750000</td>
    </tr>
    <tr>
      <th>84935</th>
      <td> umzsbhpkrgo</td>
      <td> dga</td>
      <td> 11</td>
      <td> 3.459432</td>
    </tr>
    <tr>
      <th>84936</th>
      <td> umzuyjrfwyf</td>
      <td> dga</td>
      <td> 11</td>
      <td> 2.913977</td>
    </tr>
  </tbody>
</table>
</div>



## Lets plot some stuff!


```
# Boxplots show you the distribution of the data (spread).
# http://en.wikipedia.org/wiki/Box_plot

# Plot the length and entropy of domains
all_domains.boxplot('length','class')
pylab.ylabel('Domain Length')
all_domains.boxplot('entropy','class')
pylab.ylabel('Domain Entropy')
```




    <matplotlib.text.Text at 0x10ce00b50>




    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_19_1.png">
    



    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_19_2.png">
    



```
# Split the classes up so we can set colors, size, labels
cond = all_domains['class'] == 'dga'
dga = all_domains[cond]
alexa = all_domains[~cond]
plt.scatter(alexa['length'], alexa['entropy'], s=140, c='#aaaaff', label='Alexa', alpha=.2)
plt.scatter(dga['length'], dga['entropy'], s=40, c='r', label='DGA', alpha=.3)
plt.legend()
pylab.xlabel('Domain Length')
pylab.ylabel('Domain Entropy')

# Below you can see that our DGA domains do tend to have higher entropy than Alexa on average.
```




    <matplotlib.text.Text at 0x10680ff50>




    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_20_1.png">
    



```
# Lets look at the types of domains that have entropy higher than 4
high_entropy_domains = all_domains[all_domains['entropy'] > 4]
print 'Num Domains above 4 entropy: %.2f%% %d (out of %d)' % \
            (100.0*high_entropy_domains.shape[0]/all_domains.shape[0],high_entropy_domains.shape[0],all_domains.shape[0])
print "Num high entropy legit: %d" % high_entropy_domains[high_entropy_domains['class']=='legit'].shape[0]
print "Num high entropy DGA: %d" % high_entropy_domains[high_entropy_domains['class']=='dga'].shape[0]
high_entropy_domains[high_entropy_domains['class']=='legit'].head()

# Looking at the results below, we do see that there are more domains
# in the DGA group that are high entropy but only a small percentage
# of the domains are in that high entropy range...
```

    Num Domains above 4 entropy: 0.57% 361 (out of 63294)
    Num high entropy legit: 3
    Num high entropy DGA: 358





<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>29392</th>
      <td>             theukwebdesigncompany</td>
      <td> legit</td>
      <td> 21</td>
      <td> 4.070656</td>
    </tr>
    <tr>
      <th>37378</th>
      <td> texaswithlove1982-amomentlikethis</td>
      <td> legit</td>
      <td> 33</td>
      <td> 4.051822</td>
    </tr>
    <tr>
      <th>55073</th>
      <td>        congresomundialjjrperu2009</td>
      <td> legit</td>
      <td> 26</td>
      <td> 4.056021</td>
    </tr>
  </tbody>
</table>
</div>




```
high_entropy_domains[high_entropy_domains['class']=='dga'].head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>82558</th>
      <td>     a17btkyb38gxe41pwd50nxmzjxiwjwdwfrp52</td>
      <td> dga</td>
      <td> 37</td>
      <td> 4.540402</td>
    </tr>
    <tr>
      <th>82559</th>
      <td>   a17c49l68ntkqnuhvkrmyb28fubvn30e31g43dq</td>
      <td> dga</td>
      <td> 39</td>
      <td> 4.631305</td>
    </tr>
    <tr>
      <th>82560</th>
      <td>     a17d60gtnxk47gskti15izhvlviyksh64nqkz</td>
      <td> dga</td>
      <td> 37</td>
      <td> 4.270132</td>
    </tr>
    <tr>
      <th>82561</th>
      <td>    a17erpzfzh64c69csi35bqgvp52drita67jzmy</td>
      <td> dga</td>
      <td> 38</td>
      <td> 4.629249</td>
    </tr>
    <tr>
      <th>82562</th>
      <td> a17fro51oyk67b18ksfzoti55j36p32o11fvc29cr</td>
      <td> dga</td>
      <td> 41</td>
      <td> 4.305859</td>
    </tr>
  </tbody>
</table>
</div>




```
# In preparation for using scikit learn we're just going to use
# some handles that help take us from pandas land to scikit land

# List of feature vectors (scikit learn uses 'X' for the matrix of feature vectors)
X = all_domains.as_matrix(['length', 'entropy'])

# Labels (scikit learn uses 'y' for classification labels)
y = np.array(all_domains['class'].tolist()) # Yes, this is weird but it needs 
                                            # to be an np.array of strings
```


```
# Random Forest is a popular ensemble machine learning classifier.
# http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestClassifier.html
#
import sklearn.ensemble
clf = sklearn.ensemble.RandomForestClassifier(n_estimators=20, compute_importances=True) # Trees in the forest
```


```
# Now we can use scikit learn's cross validation to assess predictive performance.
scores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=5, n_jobs=4)
print scores
```

    [ 0.9688759   0.96784896  0.96729599  0.96753298  0.96887344]



```
# Wow 96% accurate! At this point we could claim success and we'd be gigantic morons...
# Recall that we have ~100k 'legit' domains and only 3.5k DGA domains
# So a classifier that marked everything as legit would be about
# 96% accurate....

# So we dive in a bit and look at the predictive performance more deeply.

# Train on a 80/20 split
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
```


```
# Now plot the results of the 80/20 split in a confusion matrix
from sklearn.metrics import confusion_matrix
labels = ['legit', 'dga']
cm = confusion_matrix(y_test, y_pred, labels)

def plot_cm(cm, labels):
    
    # Compute percentanges
    percent = (cm*100.0)/np.array(np.matrix(cm.sum(axis=1)).T)  # Derp, I'm sure there's a better way
    
    print 'Confusion Matrix Stats'
    for i, label_i in enumerate(labels):
        for j, label_j in enumerate(labels):
            print "%s/%s: %.2f%% (%d/%d)" % (label_i, label_j, (percent[i][j]), cm[i][j], cm[i].sum())

    # Show confusion matrix
    # Thanks kermit666 from stackoverflow :)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.grid(b=False)
    cax = ax.matshow(percent, cmap='coolwarm')
    pylab.title('Confusion matrix of the classifier')
    fig.colorbar(cax)
    ax.set_xticklabels([''] + labels)
    ax.set_yticklabels([''] + labels)
    pylab.xlabel('Predicted')
    pylab.ylabel('True')
    pylab.show()

plot_cm(cm, labels)

# We can see below that our suspicions were correct and the classifier is
# marking almost everything as Alexa. We FAIL.. science is hard... lets go drinking....
```

    Confusion Matrix Stats
    legit/legit: 99.89% (12152/12165)
    legit/dga: 0.11% (13/12165)
    dga/legit: 80.16% (396/494)
    dga/dga: 19.84% (98/494)



    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_27_1.png">
    



```
# Well our Mom told us we were still cool.. so with that encouragement we're
# going to compute NGrams for every Alexa domain and see if we can use the
# NGrams to help us better differentiate and mark DGA domains...

# Scikit learn has a nice NGram generator that can generate either char NGrams or word NGrams (we're using char).
# Parameters: 
#       - ngram_range=(3,5)  # Give me all ngrams of length 3, 4, and 5
#       - min_df=1e-4        # Minimumum document frequency. At 1e-4 we're saying give us NGrams that 
#                            # happen in at least .1% of the domains (so for 100k... at least 100 domains)
alexa_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-4, max_df=1.0)
```


```
# I'm SURE there's a better way to store all the counts but not sure...
# At least the min_df parameters has already done some thresholding
counts_matrix = alexa_vc.fit_transform(alexa_dataframe['domain'])
alexa_counts = np.log10(counts_matrix.sum(axis=0).getA1())
ngrams_list = alexa_vc.get_feature_names()
```


```
# For fun sort it and show it
import operator
_sorted_ngrams = sorted(zip(ngrams_list, alexa_counts), key=operator.itemgetter(1), reverse=True)
print 'Alexa NGrams: %d' % len(_sorted_ngrams)
for ngram, count in _sorted_ngrams[:10]:
    print ngram, count
```

    Alexa NGrams: 27012
    ing 3.40001963507
    lin 3.3818368
    ine 3.35295391171
    tor 3.22349594096
    ter 3.21827285357
    ion 3.20411998266
    ent 3.18184358794
    por 3.1562461904
    the 3.15228834438
    ree 3.11693964655



```
# We're also going to throw in a bunch of dictionary words
word_dataframe = pd.read_csv('data/words.txt', names=['word'], header=None, dtype={'word': np.str}, encoding='utf-8')

# Cleanup words from dictionary
word_dataframe = word_dataframe[word_dataframe['word'].map(lambda x: str(x).isalpha())]
word_dataframe = word_dataframe.applymap(lambda x: str(x).strip().lower())
word_dataframe = word_dataframe.dropna()
word_dataframe = word_dataframe.drop_duplicates()
word_dataframe.head(10)
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>37</th>
      <td>      a</td>
    </tr>
    <tr>
      <th>48</th>
      <td>     aa</td>
    </tr>
    <tr>
      <th>51</th>
      <td>    aaa</td>
    </tr>
    <tr>
      <th>53</th>
      <td>   aaaa</td>
    </tr>
    <tr>
      <th>54</th>
      <td> aaaaaa</td>
    </tr>
    <tr>
      <th>55</th>
      <td>   aaal</td>
    </tr>
    <tr>
      <th>56</th>
      <td>   aaas</td>
    </tr>
    <tr>
      <th>57</th>
      <td> aaberg</td>
    </tr>
    <tr>
      <th>58</th>
      <td> aachen</td>
    </tr>
    <tr>
      <th>59</th>
      <td>    aae</td>
    </tr>
  </tbody>
</table>
</div>




```
# Now compute NGrams on the dictionary words
# Same logic as above...
dict_vc = sklearn.feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-5, max_df=1.0)
counts_matrix = dict_vc.fit_transform(word_dataframe['word'])
dict_counts = np.log10(counts_matrix.sum(axis=0).getA1())
ngrams_list = dict_vc.get_feature_names()
```


```
# For fun sort it and show it
import operator
_sorted_ngrams = sorted(zip(ngrams_list, dict_counts), key=operator.itemgetter(1), reverse=True)
print 'Word NGrams: %d' % len(_sorted_ngrams)
for ngram, count in _sorted_ngrams[:10]:
    print ngram, count
```

    Word NGrams: 142275
    ing 4.38730082245
    ess 4.20487933376
    ati 4.19334725639
    ion 4.16503647999
    ter 4.16241503611
    nes 4.11250445877
    tio 4.07682242334
    ate 4.07236020396
    ent 4.06963110262
    tion 4.04960561259



```
# We use the transform method of the CountVectorizer to form a vector
# of ngrams contained in the domain, that vector is than multiplied
# by the counts vector (which is a column sum of the count matrix).
def ngram_count(domain):
    alexa_match = alexa_counts * alexa_vc.transform([domain]).T  # Woot vector multiply and transpose Woo Hoo!
    dict_match = dict_counts * dict_vc.transform([domain]).T
    print '%s Alexa match:%d Dict match: %d' % (domain, alexa_match, dict_match)

# Examples:
ngram_count('google')
ngram_count('facebook')
ngram_count('1cb8a5f36f')
ngram_count('pterodactylfarts')
ngram_count('ptes9dro-dwacty2lfa5rrts')
ngram_count('beyonce')
ngram_count('bey666on4ce')
```

    google Alexa match:17 Dict match: 14
    facebook Alexa match:30 Dict match: 27
    1cb8a5f36f Alexa match:0 Dict match: 0
    pterodactylfarts Alexa match:34 Dict match: 77
    ptes9dro-dwacty2lfa5rrts Alexa match:19 Dict match: 28
    beyonce Alexa match:15 Dict match: 16
    bey666on4ce Alexa match:2 Dict match: 1



```
# Compute NGram matches for all the domains and add to our dataframe
all_domains['alexa_grams']= alexa_counts * alexa_vc.transform(all_domains['domain']).T 
all_domains['word_grams']= dict_counts * dict_vc.transform(all_domains['domain']).T 
all_domains.head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>    transworld</td>
      <td> legit</td>
      <td> 10</td>
      <td> 3.121928</td>
      <td> 39.051439</td>
      <td> 44.033642</td>
    </tr>
    <tr>
      <th>2</th>
      <td>     islam2all</td>
      <td> legit</td>
      <td>  9</td>
      <td> 2.419382</td>
      <td> 15.475215</td>
      <td> 17.367964</td>
    </tr>
    <tr>
      <th>3</th>
      <td>      pulitzer</td>
      <td> legit</td>
      <td>  8</td>
      <td> 3.000000</td>
      <td> 14.458222</td>
      <td> 28.441721</td>
    </tr>
    <tr>
      <th>6</th>
      <td>   danarimedia</td>
      <td> legit</td>
      <td> 11</td>
      <td> 2.663533</td>
      <td> 40.189599</td>
      <td> 54.829856</td>
    </tr>
    <tr>
      <th>7</th>
      <td> heartbreakers</td>
      <td> legit</td>
      <td> 13</td>
      <td> 2.815072</td>
      <td> 45.354321</td>
      <td> 69.734483</td>
    </tr>
  </tbody>
</table>
</div>




```
all_domains.tail()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>84932</th>
      <td> ulxxqduryvv</td>
      <td> dga</td>
      <td> 11</td>
      <td> 2.913977</td>
      <td> 3.745231</td>
      <td> 6.464859</td>
    </tr>
    <tr>
      <th>84933</th>
      <td>    ummvzhin</td>
      <td> dga</td>
      <td>  8</td>
      <td> 2.750000</td>
      <td> 6.183945</td>
      <td> 7.180022</td>
    </tr>
    <tr>
      <th>84934</th>
      <td>    umsgnwgc</td>
      <td> dga</td>
      <td>  8</td>
      <td> 2.750000</td>
      <td> 3.272306</td>
      <td> 3.847079</td>
    </tr>
    <tr>
      <th>84935</th>
      <td> umzsbhpkrgo</td>
      <td> dga</td>
      <td> 11</td>
      <td> 3.459432</td>
      <td> 1.653213</td>
      <td> 2.546543</td>
    </tr>
    <tr>
      <th>84936</th>
      <td> umzuyjrfwyf</td>
      <td> dga</td>
      <td> 11</td>
      <td> 2.913977</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
  </tbody>
</table>
</div>




```
# Use the vectorized operations of the dataframe to investigate differences
# between the alexa and word grams
all_domains['diff'] = all_domains['alexa_grams'] - all_domains['word_grams']
all_domains.sort(['diff'], ascending=True).head(10)

# The table below shows those domain names that are more 'dictionary' and less 'web'
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>63819</th>
      <td> bipolardisorderdepressionanxiety</td>
      <td> legit</td>
      <td> 32</td>
      <td> 3.616729</td>
      <td> 115.885999</td>
      <td> 193.844156</td>
      <td>-77.958157</td>
    </tr>
    <tr>
      <th>34524</th>
      <td>   stirringtroubleinternationally</td>
      <td> legit</td>
      <td> 30</td>
      <td> 3.481728</td>
      <td> 131.209086</td>
      <td> 207.204729</td>
      <td>-75.995643</td>
    </tr>
    <tr>
      <th>63954</th>
      <td> americansforresponsiblesolutions</td>
      <td> legit</td>
      <td> 32</td>
      <td> 3.667838</td>
      <td> 145.071369</td>
      <td> 218.363956</td>
      <td>-73.292587</td>
    </tr>
    <tr>
      <th>49070</th>
      <td>    channel4embarrassingillnesses</td>
      <td> legit</td>
      <td> 29</td>
      <td> 3.440070</td>
      <td>  98.201709</td>
      <td> 169.721499</td>
      <td>-71.519790</td>
    </tr>
    <tr>
      <th>5902 </th>
      <td>              pragmatismopolitico</td>
      <td> legit</td>
      <td> 19</td>
      <td> 3.326360</td>
      <td>  59.877723</td>
      <td> 121.536223</td>
      <td>-61.658500</td>
    </tr>
    <tr>
      <th>49210</th>
      <td>          egaliteetreconciliation</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.186393</td>
      <td>  92.257111</td>
      <td> 152.125325</td>
      <td>-59.868214</td>
    </tr>
    <tr>
      <th>74130</th>
      <td>          interoperabilitybridges</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.588354</td>
      <td>  93.803640</td>
      <td> 153.626312</td>
      <td>-59.822673</td>
    </tr>
    <tr>
      <th>36976</th>
      <td>           foreclosurephilippines</td>
      <td> legit</td>
      <td> 22</td>
      <td> 3.447402</td>
      <td>  72.844280</td>
      <td> 132.514638</td>
      <td>-59.670358</td>
    </tr>
    <tr>
      <th>47055</th>
      <td>        corazonindomablecapitulos</td>
      <td> legit</td>
      <td> 25</td>
      <td> 3.813661</td>
      <td>  74.706878</td>
      <td> 133.762750</td>
      <td>-59.055872</td>
    </tr>
    <tr>
      <th>70113</th>
      <td>      annamalicesissyselfhypnosis</td>
      <td> legit</td>
      <td> 27</td>
      <td> 3.429908</td>
      <td>  68.066490</td>
      <td> 126.667692</td>
      <td>-58.601201</td>
    </tr>
  </tbody>
</table>
</div>




```
all_domains.sort(['diff'], ascending=False).head(50)

# The table below shows those domain names that are more 'web' and less 'dictionary'
# Good O' web....
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>22647</th>
      <td> gay-sex-pics-porn-pictures-gay-sex-porn-gay-se...</td>
      <td> legit</td>
      <td> 56</td>
      <td> 3.661056</td>
      <td> 160.035734</td>
      <td>  85.124184</td>
      <td> 74.911550</td>
    </tr>
    <tr>
      <th>44091</th>
      <td>    article-directory-free-submission-free-content</td>
      <td> legit</td>
      <td> 46</td>
      <td> 3.786816</td>
      <td> 233.518879</td>
      <td> 188.230453</td>
      <td> 45.288426</td>
    </tr>
    <tr>
      <th>63865</th>
      <td>                         stream-free-movies-online</td>
      <td> legit</td>
      <td> 25</td>
      <td> 3.509275</td>
      <td> 118.944026</td>
      <td>  74.496915</td>
      <td> 44.447110</td>
    </tr>
    <tr>
      <th>38570</th>
      <td>                         top-bookmarking-site-list</td>
      <td> legit</td>
      <td> 25</td>
      <td> 3.723074</td>
      <td> 117.162056</td>
      <td>  74.126061</td>
      <td> 43.035995</td>
    </tr>
    <tr>
      <th>79963</th>
      <td>                         best-online-shopping-site</td>
      <td> legit</td>
      <td> 25</td>
      <td> 3.452879</td>
      <td> 122.152194</td>
      <td>  79.596640</td>
      <td> 42.555554</td>
    </tr>
    <tr>
      <th>12532</th>
      <td>                           watch-free-movie-online</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.708132</td>
      <td> 101.010995</td>
      <td>  58.943451</td>
      <td> 42.067543</td>
    </tr>
    <tr>
      <th>30198</th>
      <td>                             free-online-directory</td>
      <td> legit</td>
      <td> 21</td>
      <td> 3.403989</td>
      <td> 122.359797</td>
      <td>  80.735030</td>
      <td> 41.624767</td>
    </tr>
    <tr>
      <th>40859</th>
      <td>                     free-links-articles-directory</td>
      <td> legit</td>
      <td> 29</td>
      <td> 3.702472</td>
      <td> 152.063809</td>
      <td> 110.955361</td>
      <td> 41.108448</td>
    </tr>
    <tr>
      <th>30875</th>
      <td>                              online-web-directory</td>
      <td> legit</td>
      <td> 20</td>
      <td> 3.584184</td>
      <td> 114.439863</td>
      <td>  74.082948</td>
      <td> 40.356915</td>
    </tr>
    <tr>
      <th>79001</th>
      <td>                              web-directory-online</td>
      <td> legit</td>
      <td> 20</td>
      <td> 3.584184</td>
      <td> 114.313583</td>
      <td>  74.082948</td>
      <td> 40.230634</td>
    </tr>
    <tr>
      <th>78947</th>
      <td>                                 movie-news-online</td>
      <td> legit</td>
      <td> 17</td>
      <td> 3.175123</td>
      <td>  81.036910</td>
      <td>  41.705735</td>
      <td> 39.331174</td>
    </tr>
    <tr>
      <th>51532</th>
      <td>                               xxx-porno-sexvideos</td>
      <td> legit</td>
      <td> 19</td>
      <td> 3.260828</td>
      <td>  73.025165</td>
      <td>  35.176549</td>
      <td> 37.848617</td>
    </tr>
    <tr>
      <th>42200</th>
      <td>                              free-tv-video-online</td>
      <td> legit</td>
      <td> 20</td>
      <td> 3.284184</td>
      <td>  83.341214</td>
      <td>  45.662984</td>
      <td> 37.678230</td>
    </tr>
    <tr>
      <th>40771</th>
      <td>                           freegamesforyourwebsite</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.551191</td>
      <td> 114.291735</td>
      <td>  78.515881</td>
      <td> 35.775855</td>
    </tr>
    <tr>
      <th>58275</th>
      <td>                            free-web-mobile-themes</td>
      <td> legit</td>
      <td> 22</td>
      <td> 3.356492</td>
      <td>  88.503556</td>
      <td>  54.149725</td>
      <td> 34.353831</td>
    </tr>
    <tr>
      <th>70724</th>
      <td>                             seowebdirectoryonline</td>
      <td> legit</td>
      <td> 21</td>
      <td> 3.499228</td>
      <td> 126.111921</td>
      <td>  91.819498</td>
      <td> 34.292423</td>
    </tr>
    <tr>
      <th>28283</th>
      <td>                               download-free-games</td>
      <td> legit</td>
      <td> 19</td>
      <td> 3.576618</td>
      <td>  84.492962</td>
      <td>  50.661490</td>
      <td> 33.831472</td>
    </tr>
    <tr>
      <th>18894</th>
      <td>                           web-link-directory-site</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.729446</td>
      <td> 102.993078</td>
      <td>  69.367186</td>
      <td> 33.625893</td>
    </tr>
    <tr>
      <th>4838 </th>
      <td>                                 the-web-directory</td>
      <td> legit</td>
      <td> 17</td>
      <td> 3.454822</td>
      <td>  87.520339</td>
      <td>  54.697986</td>
      <td> 32.822353</td>
    </tr>
    <tr>
      <th>65871</th>
      <td>                           social-bookmarking-site</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.762267</td>
      <td> 116.664791</td>
      <td>  84.545021</td>
      <td> 32.119769</td>
    </tr>
    <tr>
      <th>21743</th>
      <td>                              free-links-directory</td>
      <td> legit</td>
      <td> 20</td>
      <td> 3.646439</td>
      <td> 104.050046</td>
      <td>  71.956644</td>
      <td> 32.093402</td>
    </tr>
    <tr>
      <th>74449</th>
      <td>                                 money-news-online</td>
      <td> legit</td>
      <td> 17</td>
      <td> 3.101881</td>
      <td>  77.587799</td>
      <td>  45.775375</td>
      <td> 31.812424</td>
    </tr>
    <tr>
      <th>48456</th>
      <td>                                 free-sexvideosfc2</td>
      <td> legit</td>
      <td> 17</td>
      <td> 3.381580</td>
      <td>  63.659477</td>
      <td>  31.878432</td>
      <td> 31.781045</td>
    </tr>
    <tr>
      <th>57427</th>
      <td>                           your-new-directory-site</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.555533</td>
      <td>  99.130671</td>
      <td>  67.468067</td>
      <td> 31.662605</td>
    </tr>
    <tr>
      <th>49041</th>
      <td>                        addsiteurlfreewebdirectory</td>
      <td> legit</td>
      <td> 26</td>
      <td> 3.609496</td>
      <td> 134.446230</td>
      <td> 103.178748</td>
      <td> 31.267482</td>
    </tr>
    <tr>
      <th>34821</th>
      <td>                                  own-free-website</td>
      <td> legit</td>
      <td> 16</td>
      <td> 3.250000</td>
      <td>  59.564153</td>
      <td>  28.839294</td>
      <td> 30.724859</td>
    </tr>
    <tr>
      <th>10080</th>
      <td>                                web-directory-plus</td>
      <td> legit</td>
      <td> 18</td>
      <td> 3.836592</td>
      <td>  89.030979</td>
      <td>  58.484138</td>
      <td> 30.546841</td>
    </tr>
    <tr>
      <th>43762</th>
      <td>                               web-directory-sites</td>
      <td> legit</td>
      <td> 19</td>
      <td> 3.471354</td>
      <td>  98.528255</td>
      <td>  68.088416</td>
      <td> 30.439839</td>
    </tr>
    <tr>
      <th>34811</th>
      <td>                                  free-sex-for-you</td>
      <td> legit</td>
      <td> 16</td>
      <td> 3.030639</td>
      <td>  46.653059</td>
      <td>  16.670504</td>
      <td> 29.982555</td>
    </tr>
    <tr>
      <th>21390</th>
      <td>                                online-deal-coupon</td>
      <td> legit</td>
      <td> 18</td>
      <td> 3.308271</td>
      <td>  77.862004</td>
      <td>  47.886115</td>
      <td> 29.975889</td>
    </tr>
    <tr>
      <th>48204</th>
      <td>                          acme-people-search-forum</td>
      <td> legit</td>
      <td> 24</td>
      <td> 3.553509</td>
      <td>  87.829242</td>
      <td>  57.898987</td>
      <td> 29.930255</td>
    </tr>
    <tr>
      <th>73304</th>
      <td>                                 free-webdirectory</td>
      <td> legit</td>
      <td> 17</td>
      <td> 3.337175</td>
      <td>  93.606205</td>
      <td>  63.858372</td>
      <td> 29.747833</td>
    </tr>
    <tr>
      <th>44221</th>
      <td>                                good-web-directory</td>
      <td> legit</td>
      <td> 18</td>
      <td> 3.461320</td>
      <td>  88.201881</td>
      <td>  58.629789</td>
      <td> 29.572091</td>
    </tr>
    <tr>
      <th>50633</th>
      <td>                                 all-free-download</td>
      <td> legit</td>
      <td> 17</td>
      <td> 3.219528</td>
      <td>  69.337916</td>
      <td>  39.909696</td>
      <td> 29.428220</td>
    </tr>
    <tr>
      <th>57095</th>
      <td>                               free-link-directory</td>
      <td> legit</td>
      <td> 19</td>
      <td> 3.536887</td>
      <td>  95.869062</td>
      <td>  66.507042</td>
      <td> 29.362020</td>
    </tr>
    <tr>
      <th>58652</th>
      <td>                              global-web-directory</td>
      <td> legit</td>
      <td> 20</td>
      <td> 3.721928</td>
      <td> 100.465474</td>
      <td>  71.293587</td>
      <td> 29.171887</td>
    </tr>
    <tr>
      <th>74259</th>
      <td>                                 online-games-zone</td>
      <td> legit</td>
      <td> 17</td>
      <td> 3.292770</td>
      <td>  74.987811</td>
      <td>  45.881826</td>
      <td> 29.105985</td>
    </tr>
    <tr>
      <th>77290</th>
      <td>                                  us-web-directory</td>
      <td> legit</td>
      <td> 16</td>
      <td> 3.625000</td>
      <td>  80.044863</td>
      <td>  50.969551</td>
      <td> 29.075312</td>
    </tr>
    <tr>
      <th>72128</th>
      <td>                           bookmarking-sites-lists</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.621176</td>
      <td> 115.664939</td>
      <td>  86.595393</td>
      <td> 29.069546</td>
    </tr>
    <tr>
      <th>64948</th>
      <td>                           web-marketing-directory</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.849224</td>
      <td> 125.587313</td>
      <td>  96.714227</td>
      <td> 28.873086</td>
    </tr>
    <tr>
      <th>79557</th>
      <td>                               freewebdirectory101</td>
      <td> legit</td>
      <td> 19</td>
      <td> 3.471354</td>
      <td> 100.131488</td>
      <td>  71.474824</td>
      <td> 28.656664</td>
    </tr>
    <tr>
      <th>72737</th>
      <td>                                     free-seo-news</td>
      <td> legit</td>
      <td> 13</td>
      <td> 2.777363</td>
      <td>  45.267539</td>
      <td>  17.089020</td>
      <td> 28.178520</td>
    </tr>
    <tr>
      <th>53449</th>
      <td>                               website-traffic-hog</td>
      <td> legit</td>
      <td> 19</td>
      <td> 3.721612</td>
      <td>  77.199578</td>
      <td>  49.156126</td>
      <td> 28.043452</td>
    </tr>
    <tr>
      <th>50837</th>
      <td>                              myonlinewebdirectory</td>
      <td> legit</td>
      <td> 20</td>
      <td> 3.584184</td>
      <td> 121.155376</td>
      <td>  93.276322</td>
      <td> 27.879054</td>
    </tr>
    <tr>
      <th>29303</th>
      <td>                           business-web-directorys</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.621176</td>
      <td> 125.854338</td>
      <td>  98.160126</td>
      <td> 27.694212</td>
    </tr>
    <tr>
      <th>41310</th>
      <td>                            free-online-submission</td>
      <td> legit</td>
      <td> 22</td>
      <td> 3.413088</td>
      <td> 113.459411</td>
      <td>  85.792712</td>
      <td> 27.666699</td>
    </tr>
    <tr>
      <th>76645</th>
      <td>                               linkdirectoryonline</td>
      <td> legit</td>
      <td> 19</td>
      <td> 3.326360</td>
      <td> 116.879367</td>
      <td>  89.392747</td>
      <td> 27.486621</td>
    </tr>
    <tr>
      <th>30430</th>
      <td>                                  online-deal-site</td>
      <td> legit</td>
      <td> 16</td>
      <td> 3.202820</td>
      <td>  68.103656</td>
      <td>  40.887484</td>
      <td> 27.216172</td>
    </tr>
    <tr>
      <th>27227</th>
      <td>                                  free-site-submit</td>
      <td> legit</td>
      <td> 16</td>
      <td> 3.202820</td>
      <td>  64.158023</td>
      <td>  37.127294</td>
      <td> 27.030729</td>
    </tr>
    <tr>
      <th>62951</th>
      <td>                          mybusiness-web-directory</td>
      <td> legit</td>
      <td> 24</td>
      <td> 3.772055</td>
      <td> 124.553982</td>
      <td>  97.538670</td>
      <td> 27.015312</td>
    </tr>
  </tbody>
</table>
</div>




```
# Lets plot some stuff!
# Here we want to see whether our new 'alexa_grams' feature can help us differentiate between Legit/DGA
cond = all_domains['class'] == 'dga'
dga = all_domains[cond]
legit = all_domains[~cond]
plt.scatter(legit['length'], legit['alexa_grams'], s=120, c='#aaaaff', label='Alexa', alpha=.1)
plt.scatter(dga['length'], dga['alexa_grams'], s=40, c='r', label='DGA', alpha=.3)
plt.legend()
pylab.xlabel('Domain Length')
pylab.ylabel('Alexa NGram Matches')
```




    <matplotlib.text.Text at 0x110c87210>




    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_39_1.png">
    



```
# Lets plot some stuff!
# Here we want to see whether our new 'alexa_grams' feature can help us differentiate between Legit/DGA
cond = all_domains['class'] == 'dga'
dga = all_domains[cond]
legit = all_domains[~cond]
plt.scatter(legit['entropy'], legit['alexa_grams'],  s=120, c='#aaaaff', label='Alexa', alpha=.2)
plt.scatter(dga['entropy'], dga['alexa_grams'], s=40, c='r', label='DGA', alpha=.3)
plt.legend()
pylab.xlabel('Domain Entropy')
pylab.ylabel('Alexa Gram Matches')
```




    <matplotlib.text.Text at 0x11058e490>




    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_40_1.png">
    



```
# Lets plot some stuff!
# Here we want to see whether our new 'word_grams' feature can help us differentiate between Legit/DGA
# Note: It doesn't look quite as good as the Alexa_grams but it might generalize better (less overfit).
cond = all_domains['class'] == 'dga'
dga = all_domains[cond]
legit = all_domains[~cond]
plt.scatter(legit['length'], legit['word_grams'],  s=120, c='#aaaaff', label='Alexa', alpha=.2)
plt.scatter(dga['length'], dga['word_grams'], s=40, c='r', label='DGA', alpha=.3)
plt.legend()
pylab.xlabel('Domain Length')
pylab.ylabel('Dictionary NGram Matches')
```




    <matplotlib.text.Text at 0x10ef8f750>




    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_41_1.png">
    



```
# Lets look at which Legit domains are scoring low on the word gram count
all_domains[(all_domains['word_grams']==0)].head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3429</th>
      <td>  dftc777</td>
      <td> legit</td>
      <td> 7</td>
      <td> 2.128085</td>
      <td> 2.707570</td>
      <td> 0</td>
      <td> 2.707570</td>
    </tr>
    <tr>
      <th>3715</th>
      <td>  5221766</td>
      <td> legit</td>
      <td> 7</td>
      <td> 2.235926</td>
      <td> 0.000000</td>
      <td> 0</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>4144</th>
      <td> 28365365</td>
      <td> legit</td>
      <td> 8</td>
      <td> 2.250000</td>
      <td> 4.050612</td>
      <td> 0</td>
      <td> 4.050612</td>
    </tr>
    <tr>
      <th>4235</th>
      <td> mm-mm-mm</td>
      <td> legit</td>
      <td> 8</td>
      <td> 0.811278</td>
      <td> 4.260668</td>
      <td> 0</td>
      <td> 4.260668</td>
    </tr>
    <tr>
      <th>4297</th>
      <td>  fzzfgjj</td>
      <td> legit</td>
      <td> 7</td>
      <td> 1.950212</td>
      <td> 0.954243</td>
      <td> 0</td>
      <td> 0.954243</td>
    </tr>
  </tbody>
</table>
</div>




```
# Okay these look kinda weird, lets use some nice Pandas functionality
# to look at some statistics around our new features.
all_domains[all_domains['class']=='legit'].describe()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td> 60897.000000</td>
      <td> 60897.000000</td>
      <td> 60897.000000</td>
      <td> 60897.000000</td>
      <td> 60897.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>    10.873032</td>
      <td>     2.930306</td>
      <td>    33.083440</td>
      <td>    40.901852</td>
      <td>    -7.818413</td>
    </tr>
    <tr>
      <th>std</th>
      <td>     3.393407</td>
      <td>     0.347134</td>
      <td>    19.233994</td>
      <td>    23.302539</td>
      <td>     9.388916</td>
    </tr>
    <tr>
      <th>min</th>
      <td>     7.000000</td>
      <td>    -0.000000</td>
      <td>     0.000000</td>
      <td>     0.000000</td>
      <td>   -77.958157</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>     8.000000</td>
      <td>     2.725481</td>
      <td>    19.136340</td>
      <td>    24.056214</td>
      <td>   -12.938013</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>    10.000000</td>
      <td>     2.947703</td>
      <td>    28.703813</td>
      <td>    36.259089</td>
      <td>    -7.108820</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>    13.000000</td>
      <td>     3.169925</td>
      <td>    42.400101</td>
      <td>    53.036218</td>
      <td>    -1.995136</td>
    </tr>
    <tr>
      <th>max</th>
      <td>    56.000000</td>
      <td>     4.070656</td>
      <td>   233.518879</td>
      <td>   233.648571</td>
      <td>    74.911550</td>
    </tr>
  </tbody>
</table>
</div>




```
# Lets look at how many domains that are both low in word_grams and alexa_grams (just plotting the max of either)
legit = all_domains[(all_domains['class']=='legit')]
max_grams = np.maximum(legit['alexa_grams'],legit['word_grams'])
ax = max_grams.hist(bins=80)
ax.figure.suptitle('Histogram of the Max NGram Score for Domains')
pylab.xlabel('Number of Domains')
pylab.ylabel('Maximum NGram Score')
```




    <matplotlib.text.Text at 0x114ee5450>




    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_44_1.png">
    



```
# Lets look at which Legit domains are scoring low on both alexa and word gram count
weird_cond = (all_domains['class']=='legit') & (all_domains['word_grams']<3) & (all_domains['alexa_grams']<2)
weird = all_domains[weird_cond]
print weird.shape[0]
weird.head(30)
```

    79





<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>85   </th>
      <td>                 9to5lol</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.235926</td>
      <td> 1.991226</td>
      <td> 2.359835</td>
      <td>-0.368609</td>
    </tr>
    <tr>
      <th>2611 </th>
      <td>                 akb48mt</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.807355</td>
      <td> 1.301030</td>
      <td> 1.041393</td>
      <td> 0.259637</td>
    </tr>
    <tr>
      <th>3715 </th>
      <td>                 5221766</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.235926</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>4297 </th>
      <td>                 fzzfgjj</td>
      <td> legit</td>
      <td>  7</td>
      <td> 1.950212</td>
      <td> 0.954243</td>
      <td> 0.000000</td>
      <td> 0.954243</td>
    </tr>
    <tr>
      <th>6045 </th>
      <td>                 crx7601</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.807355</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>8531 </th>
      <td>                 mw7zrv2</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.807355</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>10802</th>
      <td>                 jmm1818</td>
      <td> legit</td>
      <td>  7</td>
      <td> 1.950212</td>
      <td> 0.903090</td>
      <td> 0.000000</td>
      <td> 0.903090</td>
    </tr>
    <tr>
      <th>11961</th>
      <td>                 qq66699</td>
      <td> legit</td>
      <td>  7</td>
      <td> 1.556657</td>
      <td> 1.322219</td>
      <td> 0.000000</td>
      <td> 1.322219</td>
    </tr>
    <tr>
      <th>13200</th>
      <td>                 twcczhu</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 1.724276</td>
      <td> 0.000000</td>
      <td> 1.724276</td>
    </tr>
    <tr>
      <th>13756</th>
      <td>                 hljdns4</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.807355</td>
      <td> 1.724276</td>
      <td> 0.000000</td>
      <td> 1.724276</td>
    </tr>
    <tr>
      <th>14763</th>
      <td>                 6470355</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>17322</th>
      <td>                d20pfsrd</td>
      <td> legit</td>
      <td>  8</td>
      <td> 2.750000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>20591</th>
      <td>                 lgcct27</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 1.176091</td>
      <td> 0.845098</td>
      <td> 0.330993</td>
    </tr>
    <tr>
      <th>23458</th>
      <td>                 jdoqocy</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 0.000000</td>
      <td> 2.813581</td>
      <td>-2.813581</td>
    </tr>
    <tr>
      <th>24661</th>
      <td>                95178114</td>
      <td> legit</td>
      <td>  8</td>
      <td> 2.405639</td>
      <td> 1.591065</td>
      <td> 0.000000</td>
      <td> 1.591065</td>
    </tr>
    <tr>
      <th>24720</th>
      <td>                ggmmxxoo</td>
      <td> legit</td>
      <td>  8</td>
      <td> 2.000000</td>
      <td> 1.113943</td>
      <td> 0.602060</td>
      <td> 0.511883</td>
    </tr>
    <tr>
      <th>26454</th>
      <td>                 ggmm777</td>
      <td> legit</td>
      <td>  7</td>
      <td> 1.556657</td>
      <td> 1.477121</td>
      <td> 0.602060</td>
      <td> 0.875061</td>
    </tr>
    <tr>
      <th>27222</th>
      <td>                 rkg1866</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 0.954243</td>
      <td> 0.000000</td>
      <td> 0.954243</td>
    </tr>
    <tr>
      <th>27676</th>
      <td>                 1616bbs</td>
      <td> legit</td>
      <td>  7</td>
      <td> 1.950212</td>
      <td> 1.806180</td>
      <td> 1.322219</td>
      <td> 0.483961</td>
    </tr>
    <tr>
      <th>29142</th>
      <td>                 5278bbs</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 1.806180</td>
      <td> 1.322219</td>
      <td> 0.483961</td>
    </tr>
    <tr>
      <th>29551</th>
      <td>                 05tz2e9</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.807355</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>29858</th>
      <td>                 1532777</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.128085</td>
      <td> 1.477121</td>
      <td> 0.000000</td>
      <td> 1.477121</td>
    </tr>
    <tr>
      <th>30119</th>
      <td>                 5311314</td>
      <td> legit</td>
      <td>  7</td>
      <td> 1.842371</td>
      <td> 1.000000</td>
      <td> 0.000000</td>
      <td> 1.000000</td>
    </tr>
    <tr>
      <th>30290</th>
      <td>                zzgcjyzx</td>
      <td> legit</td>
      <td>  8</td>
      <td> 2.405639</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>30739</th>
      <td>             xn--g5t518j</td>
      <td> legit</td>
      <td> 11</td>
      <td> 3.095795</td>
      <td> 1.000000</td>
      <td> 0.000000</td>
      <td> 1.000000</td>
    </tr>
    <tr>
      <th>31465</th>
      <td>                 7210578</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 0.903090</td>
      <td> 0.000000</td>
      <td> 0.903090</td>
    </tr>
    <tr>
      <th>31951</th>
      <td>                 fj96336</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.235926</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>34455</th>
      <td> xn--42cgk1gc8crdb1htg3d</td>
      <td> legit</td>
      <td> 23</td>
      <td> 3.849224</td>
      <td> 1.255273</td>
      <td> 2.411620</td>
      <td>-1.156347</td>
    </tr>
    <tr>
      <th>35554</th>
      <td>                 720pmkv</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.807355</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>36166</th>
      <td>                 d4ffr55</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.235926</td>
      <td> 1.079181</td>
      <td> 2.260071</td>
      <td>-1.180890</td>
    </tr>
  </tbody>
</table>
</div>




```
# Epiphany... Alexa really may not be the best 'exemplar' set...  
#             (probably a no-shit moment for everyone else :)
#
# Discussion: If you're using these as exemplars of NOT DGA, then your probably
#             making things very hard on your machine learning algorithm.
#             Perhaps we should have two categories of Alexa domains, 'legit'
#             and a 'weird'. based on some definition of weird.
#             Looking at the entries above... we have approx 80 domains
#             that we're going to mark as 'weird'.
#
all_domains.loc[weird_cond, 'class'] = 'weird'
print all_domains['class'].value_counts()
all_domains[all_domains['class'] == 'weird'].head()
```

    legit    60818
    dga       2397
    weird       79
    dtype: int64





<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>85  </th>
      <td> 9to5lol</td>
      <td> weird</td>
      <td> 7</td>
      <td> 2.235926</td>
      <td> 1.991226</td>
      <td> 2.359835</td>
      <td>-0.368609</td>
    </tr>
    <tr>
      <th>2611</th>
      <td> akb48mt</td>
      <td> weird</td>
      <td> 7</td>
      <td> 2.807355</td>
      <td> 1.301030</td>
      <td> 1.041393</td>
      <td> 0.259637</td>
    </tr>
    <tr>
      <th>3715</th>
      <td> 5221766</td>
      <td> weird</td>
      <td> 7</td>
      <td> 2.235926</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
    <tr>
      <th>4297</th>
      <td> fzzfgjj</td>
      <td> weird</td>
      <td> 7</td>
      <td> 1.950212</td>
      <td> 0.954243</td>
      <td> 0.000000</td>
      <td> 0.954243</td>
    </tr>
    <tr>
      <th>6045</th>
      <td> crx7601</td>
      <td> weird</td>
      <td> 7</td>
      <td> 2.807355</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
    </tr>
  </tbody>
</table>
</div>




```
# Now we try our machine learning algorithm again with the new features
# Alexa and Dictionary NGrams and the exclusion of the bad exemplars.
X = all_domains.as_matrix(['length', 'entropy', 'alexa_grams', 'word_grams'])

# Labels (scikit learn uses 'y' for classification labels)
y = np.array(all_domains['class'].tolist())

# Train on a 80/20 split
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
```


```
# Now plot the results of the 80/20 split in a confusion matrix
labels = ['legit', 'weird', 'dga']
cm = confusion_matrix(y_test, y_pred, labels)
plot_cm(cm, labels)
```

    Confusion Matrix Stats
    legit/legit: 99.58% (12140/12191)
    legit/weird: 0.01% (1/12191)
    legit/dga: 0.41% (50/12191)
    weird/legit: 0.00% (0/10)
    weird/weird: 30.00% (3/10)
    weird/dga: 70.00% (7/10)
    dga/legit: 14.63% (67/458)
    dga/weird: 0.22% (1/458)
    dga/dga: 85.15% (390/458)



    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_48_1.png">
    



```
# Hun, well that seem to work 'ok', but you don't really want a classifier
# that outputs 3 classes, you'd like a classifier that flags domains as DGA or not.
# This was a path that seemed like a good idea until it wasn't....
```


```
# Perhaps we will just exclude the weird class from our ML training
not_weird = all_domains[all_domains['class'] != 'weird']
X = not_weird.as_matrix(['length', 'entropy', 'alexa_grams', 'word_grams'])

# Labels (scikit learn uses 'y' for classification labels)
y = np.array(not_weird['class'].tolist())

# Train on a 80/20 split
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
```


```
# Now plot the results of the 80/20 split in a confusion matrix
labels = ['legit', 'dga']
cm = confusion_matrix(y_test, y_pred, labels)
plot_cm(cm, labels) 
```

    Confusion Matrix Stats
    legit/legit: 99.56% (12111/12165)
    legit/dga: 0.44% (54/12165)
    dga/legit: 17.99% (86/478)
    dga/dga: 82.01% (392/478)



    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_51_1.png">
    



```
# Well it's definitely better.. but haven't we just cheated by removing
# the weird domains?  Well perhaps, but on some level we're removing
# outliers that are bad exemplars. So to validate that the model is still
# doing the right thing lets try our new model prediction on our hold out sets.

# First train on the whole thing before looking at prediction performance
clf.fit(X, y)

# Pull together our hold out set
hold_out_domains = pd.concat([hold_out_alexa, hold_out_dga], ignore_index=True)

# Add a length field for the domain
hold_out_domains['length'] = [len(x) for x in hold_out_domains['domain']]
hold_out_domains = hold_out_domains[hold_out_domains['length'] > 6]

# Add a entropy field for the domain
hold_out_domains['entropy'] = [entropy(x) for x in hold_out_domains['domain']]

# Compute NGram matches for all the domains and add to our dataframe
hold_out_domains['alexa_grams']= alexa_counts * alexa_vc.transform(hold_out_domains['domain']).T
hold_out_domains['word_grams']= dict_counts * dict_vc.transform(hold_out_domains['domain']).T

hold_out_domains.head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>      alcatelonetouch</td>
      <td> legit</td>
      <td> 15</td>
      <td> 3.106891</td>
      <td> 49.001768</td>
      <td> 79.015001</td>
    </tr>
    <tr>
      <th>1</th>
      <td> optumhealthfinancial</td>
      <td> legit</td>
      <td> 20</td>
      <td> 3.584184</td>
      <td> 68.667084</td>
      <td> 87.158661</td>
    </tr>
    <tr>
      <th>4</th>
      <td>   elderscrollsonline</td>
      <td> legit</td>
      <td> 18</td>
      <td> 3.016876</td>
      <td> 76.441834</td>
      <td> 94.462092</td>
    </tr>
    <tr>
      <th>5</th>
      <td>              mobango</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 18.020832</td>
      <td> 22.072036</td>
    </tr>
    <tr>
      <th>6</th>
      <td>              costaud</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.807355</td>
      <td> 16.037393</td>
      <td> 25.008755</td>
    </tr>
  </tbody>
</table>
</div>




```
# List of feature vectors (scikit learn uses 'X' for the matrix of feature vectors)
hold_X = hold_out_domains.as_matrix(['length', 'entropy', 'alexa_grams', 'word_grams'])

# Labels (scikit learn uses 'y' for classification labels)
hold_y = np.array(hold_out_domains['class'].tolist())

# Now run through the predictive model
hold_y_pred = clf.predict(hold_X)

# Add the prediction array to the dataframe
hold_out_domains['pred'] = hold_y_pred

# Now plot the results
labels = ['legit', 'dga']
cm = confusion_matrix(hold_y, hold_y_pred, labels)
plot_cm(cm, labels) 
```

    Confusion Matrix Stats
    legit/legit: 99.51% (6713/6746)
    legit/dga: 0.49% (33/6746)
    dga/legit: 15.73% (42/267)
    dga/dga: 84.27% (225/267)



    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_53_1.png">
    



```
# Okay so on our 10% hold out set of 10k domains about ~100 domains were mis-classified
# at this point we're made some good progress so we're going to claim success :)
#       - Out of 10k domains 100 were mismarked
#       - false positives (Alexa marked as DGA) = ~0.6%
#       - about 80% of the DGA are getting marked

# Note: Alexa 1M results on the 10% hold out (100k domains) were in the same ballpark 
#       - Out of 100k domains 432 were mismarked
#       - false positives (Alexa marked as DGA) = 0.4%
#       - about 70% of the DGA are getting marked

# Now were going to just do some post analysis on how the ML algorithm performed.

# Lets look at a couple of plots to see which domains were misclassified.
# Looking at Length vs. Alexa NGrams
fp_cond = ((hold_out_domains['class'] == 'legit') & (hold_out_domains['pred']=='dga'))
fp = hold_out_domains[fp_cond]
fn_cond = ((hold_out_domains['class'] == 'dba') & (hold_out_domains['pred']=='legit'))
fn = hold_out_domains[fn_cond]
okay = hold_out_domains[hold_out_domains['class'] == hold_out_domains['pred']]
plt.scatter(okay['length'], okay['alexa_grams'], s=100,  c='#aaaaff', label='Okay', alpha=.1)
plt.scatter(fp['length'], fp['alexa_grams'], s=40, c='r', label='False Positive', alpha=.5)
plt.legend()
pylab.xlabel('Domain Length')
pylab.ylabel('Alexa NGram Matches')
```




    <matplotlib.text.Text at 0x115ea7390>




    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_54_1.png">
    



```
# Looking at Length vs. Dictionary NGrams
cond = (hold_out_domains['class'] != hold_out_domains['pred'])
misclassified = hold_out_domains[cond]
okay = hold_out_domains[~cond]
plt.scatter(okay['length'], okay['word_grams'], s=100,  c='#aaaaff', label='Okay', alpha=.2)
plt.scatter(misclassified['length'], misclassified['word_grams'], s=40, c='r', label='Misclassified', alpha=.3)
plt.legend()
pylab.xlabel('Domain Length')
pylab.ylabel('Dictionary NGram Matches')
```




    <matplotlib.text.Text at 0x115e9b990>




    
<img src="/assets/images/DGA_Domain_Detection_files/DGA_Domain_Detection_55_1.png">
    



```
misclassified.head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>pred</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>896 </th>
      <td>    dom2-fan</td>
      <td> legit</td>
      <td>  8</td>
      <td> 3.000000</td>
      <td> 6.568955</td>
      <td> 5.656685</td>
      <td> dga</td>
    </tr>
    <tr>
      <th>1296</th>
      <td> mm8mm8-6642</td>
      <td> legit</td>
      <td> 11</td>
      <td> 2.368523</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> dga</td>
    </tr>
    <tr>
      <th>1378</th>
      <td>     4390208</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> dga</td>
    </tr>
    <tr>
      <th>1514</th>
      <td>     sqrt121</td>
      <td> legit</td>
      <td>  7</td>
      <td> 2.521641</td>
      <td> 0.000000</td>
      <td> 0.000000</td>
      <td> dga</td>
    </tr>
    <tr>
      <th>1687</th>
      <td> 02022222222</td>
      <td> legit</td>
      <td> 11</td>
      <td> 0.684038</td>
      <td> 0.903090</td>
      <td> 0.000000</td>
      <td> dga</td>
    </tr>
  </tbody>
</table>
</div>




```
misclassified[misclassified['class'] == 'dga'].head()
```




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>domain</th>
      <th>class</th>
      <th>length</th>
      <th>entropy</th>
      <th>alexa_grams</th>
      <th>word_grams</th>
      <th>pred</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9184</th>
      <td> usbiezgac</td>
      <td> dga</td>
      <td> 9</td>
      <td> 3.169925</td>
      <td>  7.825928</td>
      <td>  9.172547</td>
      <td> legit</td>
    </tr>
    <tr>
      <th>9185</th>
      <td>  ushcnewo</td>
      <td> dga</td>
      <td> 8</td>
      <td> 3.000000</td>
      <td> 12.265642</td>
      <td> 13.904812</td>
      <td> legit</td>
    </tr>
    <tr>
      <th>9187</th>
      <td>  usnspdph</td>
      <td> dga</td>
      <td> 8</td>
      <td> 2.500000</td>
      <td>  5.182278</td>
      <td>  6.556287</td>
      <td> legit</td>
    </tr>
    <tr>
      <th>9190</th>
      <td>   utamehz</td>
      <td> dga</td>
      <td> 7</td>
      <td> 2.807355</td>
      <td> 10.741352</td>
      <td> 14.733893</td>
      <td> legit</td>
    </tr>
    <tr>
      <th>9192</th>
      <td>  utfowept</td>
      <td> dga</td>
      <td> 8</td>
      <td> 2.750000</td>
      <td>  7.095911</td>
      <td> 17.416355</td>
      <td> legit</td>
    </tr>
  </tbody>
</table>
</div>




```
# We can also look at what features the learning algorithm thought were the most important
importances = zip(['length', 'entropy', 'alexa_grams', 'word_grams'], clf.feature_importances_)
importances

# From the list below we see our feature importance scores. There's a lot of feature selection,
# sensitivity study, etc stuff that you could do if you wanted at this point.
```




    [('length', 0.13110737655160343),
     ('entropy', 0.15589784074688856),
     ('alexa_grams', 0.48657282029928439),
     ('word_grams', 0.22642196240222362)]




```
# Discussion for how to use the resulting models.
# Typically Machine Learning comes in two phases
#    - Training of the Model
#    - Evaluation of new observations against the Model
# This notebook is about exploration of the data and training the model.
# After you have a model that you are satisfied with, just 'pickle' it
# at the end of the your training script and then in a separate
# evaluation script 'unpickle' it and evaluate/score new observations
# coming in (through a file, or ZeroMQ, or whatever...)
#
# In this case we'd have to pickle the RandomForest classifier
# and the two vectorizing transforms (alexa_grams and word_grams).
# See 'test_it' below for how to use them in evaluation mode.


# test_it shows how to do evaluation, also fun for manual testing below :)
def test_it(domain):
    
    _alexa_match = alexa_counts * alexa_vc.transform([domain]).T  # Woot matrix multiply and transpose Woo Hoo!
    _dict_match = dict_counts * dict_vc.transform([domain]).T
    _X = [len(domain), entropy(domain), _alexa_match, _dict_match]
    print '%s : %s' % (domain, clf.predict(_X)[0])
```


```
# Examples (feel free to change these and see the results!)
test_it('google')
test_it('google88')
test_it('facebook')
test_it('1cb8a5f36f')
test_it('pterodactylfarts')
test_it('ptes9dro-dwacty2lfa5rrts')
test_it('beyonce')
test_it('bey666on4ce')
test_it('supersexy')
test_it('yourmomissohotinthesummertime')
test_it('35-sdf-09jq43r')
test_it('clicksecurity')
```

    google : legit
    google88 : legit
    facebook : legit
    1cb8a5f36f : dga
    pterodactylfarts : legit
    ptes9dro-dwacty2lfa5rrts : dga
    beyonce : legit
    bey666on4ce : dga
    supersexy : legit
    yourmomissohotinthesummertime : legit
    35-sdf-09jq43r : dga
    clicksecurity : legit


### Conclusions:
The combination of IPython, Pandas and Scikit Learn let us pull in some junky data, clean it up, plot it, understand it and slap it with some machine learning!

Clearly a lot more formality could be used, plotting learning curves, adjusting for overfitting, feature selection, on and on... there are some really great machine learning resources that cover this deeper material. In particular we highly recommend the work and presentations of Olivier Grisel at INRIA Saclay. http://ogrisel.com/

Some papers on detecting DGA domains:
 
 - S. Yadav, A. K. K. Reddy, A. L. N. Reddy, and S. Ranjan, Detecting algorithmically generated malicious domain names, presented at the the 10th annual conference, New York, New York, USA, 2010, pp. 4861. [http://conferences.sigcomm.org/imc/2010/papers/p48.pdf]
 - S. Yadav, A. K. K. Reddy, A. L. N. Reddy, and S. Ranjan, Detecting algorithmically generated domain-flux attacks with DNS traffic analysis, IEEE/ACM Transactions on Networking (TON, vol. 20, no. 5, Oct. 2012.
 - A. Reddy, Detecting Networks Employing Algorithmically Generated Domain Names, 2010.
 - Z. Wei-wei and G. Qian, Detecting Machine Generated Domain Names Based on Morpheme Features, 2013.
 - P. Barthakur, M. Dahal, and M. K. Ghose, An Efficient Machine Learning Based Classification Scheme for Detecting Distributed Command & Control Traffic of P2P Botnets, International Journal of Modern , 2013.
